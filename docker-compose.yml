services:
  libretranslate:
    image: libretranslate/libretranslate:latest
    ports:
      - "5000:5000"
    environment:
      - DEFAULT_SOURCE=en
      - DEFAULT_TARGET=nb
    volumes:
      - libre-data:/data

  nllb:
    build:
      context: ./nllb
    environment:
      # Force CPU in this variant
      - NLLB_DEVICE=cpu
      # Prefer local model folder (from .env), fallback path 'current' if not set
      - NLLB_MODEL_DIR=/models/${NLLB_MODEL_LOCAL:-current}
      # Keep HF cache
      - HF_HOME=/models_cache
      # Harmless on CPU
      - TORCH_CUDA_ALLOC_CONF=max_split_size_mb:64
      # Also pass through hub id fallback from .env
      - NLLB_MODEL=${NLLB_MODEL}
    volumes:
      # Cache volume (keep as you had)
      - ./models_cache:/models_cache
      # Local models (read-only)
      - ./models:/models:ro
    ports:
      - "127.0.0.1:6100:6100"
    shm_size: "2gb"
    restart: unless-stopped

  app:
    build:
      context: ./app
      dockerfile: Dockerfile
    container_name: srtxlate_app
    env_file:
      - .env
    ports:
      - "8080:8080"
    depends_on:
      - libretranslate
      - nllb
    environment:
      - DEFAULT_SOURCE=en
      - DEFAULT_TARGET=nb
      - DEFAULT_ENGINE=auto
      - NLLB_ENDPOINT=http://nllb:6100
      - LIBRE_ENDPOINT=http://libretranslate:5000

volumes:
  libre-data:
